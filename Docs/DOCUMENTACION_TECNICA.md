# üìö DOCUMENTACI√ìN T√âCNICA COMPLETA
## Sistema de Chatbot Veterinario con Red Neuronal y An√°lisis Predictivo

---

## üìã Tabla de Contenidos

1. [Arquitectura del Sistema](#arquitectura-del-sistema)
2. [M√≥dulo: Database (database.py)](#m√≥dulo-database)
3. [M√≥dulo: Predictor (predictor.py)](#m√≥dulo-predictor)
4. [M√≥dulo: Chatbot (chatbot.py)](#m√≥dulo-chatbot)
5. [Entrenamiento del Chatbot](#entrenamiento-del-chatbot)
6. [API REST (api.py)](#api-rest)
7. [Flujo de Datos](#flujo-de-datos)
8. [C√≥mo Entrenar los Modelos](#c√≥mo-entrenar-los-modelos)

---

## üèóÔ∏è Arquitectura del Sistema

El sistema est√° compuesto por 4 componentes principales:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FRONTEND REACT                           ‚îÇ
‚îÇ                   (Tu aplicaci√≥n web)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP Requests
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      API REST (api.py)                          ‚îÇ
‚îÇ              FastAPI - Endpoints HTTP/JSON                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                     ‚îÇ                  ‚îÇ
        ‚ñº                     ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CHATBOT       ‚îÇ  ‚îÇ PREDICTOR       ‚îÇ  ‚îÇ DATABASE         ‚îÇ
‚îÇ chatbot.py    ‚îÇ  ‚îÇ predictor.py    ‚îÇ  ‚îÇ database.py      ‚îÇ
‚îÇ               ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ Red Neuronal  ‚îÇ  ‚îÇ Red Neuronal    ‚îÇ  ‚îÇ PostgreSQL       ‚îÇ
‚îÇ LSTM          ‚îÇ  ‚îÇ Dense           ‚îÇ  ‚îÇ Consultas SQL    ‚îÇ
‚îÇ (Intenciones) ‚îÇ  ‚îÇ (Predicciones)  ‚îÇ  ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                     ‚îÇ                  ‚îÇ
        ‚ñº                     ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Datos         ‚îÇ  ‚îÇ Modelos         ‚îÇ  ‚îÇ Railway DB       ‚îÇ
‚îÇ Veterinarios  ‚îÇ  ‚îÇ Predictivos     ‚îÇ  ‚îÇ PostgreSQL       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de una Petici√≥n:

1. **Frontend** env√≠a petici√≥n HTTP a la API
2. **API** recibe la petici√≥n y llama al m√≥dulo correspondiente
3. **M√≥dulo** procesa (chatbot, predictor o database)
4. **M√≥dulo** retorna resultado a la API
5. **API** formatea y env√≠a respuesta JSON al Frontend

---

## üìä M√≥dulo: Database (database.py)

### Prop√≥sito
Gestiona TODAS las conexiones y consultas a la base de datos PostgreSQL.

### Clase Principal: `PetStoreDatabase`

```python
class PetStoreDatabase:
    def __init__(self):
        self.conn = None
        self.conectar()
```

#### M√©todo: `conectar()`

**QU√â HACE:**
- Establece conexi√≥n a PostgreSQL usando credenciales de `config.py`
- Usa psycopg2 para la conexi√≥n

**C√ìDIGO:**
```python
def conectar(self):
    try:
        self.conn = psycopg2.connect(**DB_CONFIG)
        # DB_CONFIG contiene: host, port, database, user, password
        logger.info("‚úÖ Conexi√≥n exitosa")
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        raise
```

**POR QU√â AS√ç:**
- psycopg2 es el driver est√°ndar para PostgreSQL en Python
- El try/except maneja errores de conexi√≥n gracefully
- Logger registra el estado para debugging

---

#### M√©todo: `ejecutar_query(query, params)`

**QU√â HACE:**
- Ejecuta cualquier consulta SQL y retorna un DataFrame de pandas

**C√ìDIGO:**
```python
def ejecutar_query(self, query: str, params: tuple = None) -> pd.DataFrame:
    try:
        if params:
            df = pd.read_sql(query, self.conn, params=params)
        else:
            df = pd.read_sql(query, self.conn)
        return df
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return pd.DataFrame()
```

**POR QU√â AS√ç:**
- pandas.read_sql() convierte autom√°ticamente resultados SQL a DataFrame
- params previene SQL injection
- Retorna DataFrame vac√≠o en caso de error (evita crashes)

---

#### M√©todo: `obtener_dataset_completo()`

**QU√â HACE:**
- Obtiene TODOS los datos necesarios para entrenar modelos de predicci√≥n
- Une m√∫ltiples tablas (appointment, pet, service, client)

**C√ìDIGO SIMPLIFICADO:**
```python
def obtener_dataset_completo(self) -> pd.DataFrame:
    query = """
    SELECT 
        -- Caracter√≠sticas temporales
        EXTRACT(YEAR FROM a.fecha_hora) AS a√±o,
        EXTRACT(MONTH FROM a.fecha_hora) AS mes,
        EXTRACT(DAY FROM a.fecha_hora) AS dia,
        EXTRACT(DOW FROM a.fecha_hora) AS dia_semana,
        EXTRACT(HOUR FROM a.fecha_hora) AS hora,
        
        -- Caracter√≠sticas de servicio
        s.nombre AS servicio,
        s.precio AS precio_servicio,
        s.duracion_minutos,
        
        -- Caracter√≠sticas de mascota
        p.tipo AS tipo_mascota,
        p.raza,
        p.edad AS edad_mascota,
        p.sexo AS sexo_mascota,
        
        -- Target (lo que queremos predecir)
        CASE WHEN a.estado = 'COMPLETADA' THEN 1 ELSE 0 END AS asistio
    FROM appointment a
    JOIN service s ON a.service_id = s.service_id
    JOIN pet p ON a.pet_id = p.pet_id
    WHERE a.activo = true
    ORDER BY a.fecha_hora DESC;
    """
    return self.ejecutar_query(query)
```

**POR QU√â ESTA CONSULTA:**
- **EXTRACT()**: Convierte fecha/hora en features num√©ricos √∫tiles para ML
- **JOIN**: Une tablas para obtener informaci√≥n completa
- **CASE WHEN**: Crea variable binaria (0/1) para clasificaci√≥n
- **WHERE activo = true**: Solo datos v√°lidos

**FEATURES EXTRA√çDOS:**
- **Temporales**: a√±o, mes, d√≠a, d√≠a_semana, hora
- **Servicio**: nombre, precio, duraci√≥n
- **Mascota**: tipo, raza, edad, sexo
- **Target**: asisti√≥ o no (1/0)

---

#### M√©todo: `obtener_tipos_mascota_mas_comunes()`

**QU√â HACE:**
- Analiza cu√°ntas mascotas hay de cada tipo
- Calcula porcentajes y promedios

**C√ìDIGO:**
```python
def obtener_tipos_mascota_mas_comunes(self) -> pd.DataFrame:
    query = """
    SELECT 
        p.tipo AS tipo_mascota,
        COUNT(DISTINCT p.pet_id) AS total_mascotas,
        COUNT(a.appointment_id) AS total_citas,
        ROUND(COUNT(a.appointment_id)::numeric / 
              NULLIF(COUNT(DISTINCT p.pet_id), 0), 2) AS promedio_citas,
        ROUND(COUNT(DISTINCT p.pet_id)::numeric * 100.0 / 
              (SELECT COUNT(*) FROM pet WHERE activo = true), 2) AS porcentaje
    FROM pet p
    LEFT JOIN appointment a ON p.pet_id = a.pet_id AND a.activo = true
    WHERE p.activo = true
    GROUP BY p.tipo
    ORDER BY total_mascotas DESC;
    """
    return self.ejecutar_query(query)
```

**AN√ÅLISIS DEL SQL:**
- **COUNT(DISTINCT p.pet_id)**: Cuenta mascotas √∫nicas (no duplicados)
- **LEFT JOIN**: Incluye mascotas aunque no tengan citas
- **NULLIF()**: Evita divisi√≥n por cero
- **Subquery**: Calcula porcentaje del total
- **GROUP BY**: Agrupa por tipo de mascota

**RESULTADO:**
```
tipo_mascota | total_mascotas | total_citas | promedio_citas | porcentaje
Perro        | 250            | 450         | 1.80           | 45.5
Gato         | 180            | 320         | 1.77           | 32.7
Ave          | 70             | 95          | 1.36           | 12.7
```

---

#### M√©todo: `obtener_dias_con_mas_atencion()`

**QU√â HACE:**
- Analiza qu√© d√≠as de la semana tienen m√°s citas
- Calcula tasas de asistencia

**C√ìDIGO:**
```python
def obtener_dias_con_mas_atencion(self) -> pd.DataFrame:
    query = """
    SELECT 
        CASE EXTRACT(DOW FROM a.fecha_hora)
            WHEN 0 THEN 'Domingo'
            WHEN 1 THEN 'Lunes'
            WHEN 2 THEN 'Martes'
            ...
        END AS dia_semana,
        COUNT(a.appointment_id) AS total_citas,
        COUNT(CASE WHEN a.estado = 'COMPLETADA' THEN 1 END) AS completadas,
        COUNT(CASE WHEN a.estado = 'CANCELADA' THEN 1 END) AS canceladas,
        ROUND(COUNT(CASE WHEN a.estado = 'COMPLETADA' THEN 1 END)::numeric * 100.0 / 
              NULLIF(COUNT(a.appointment_id), 0), 2) AS tasa_asistencia
    FROM appointment a
    WHERE a.activo = true
    GROUP BY EXTRACT(DOW FROM a.fecha_hora)
    ORDER BY numero_dia;
    """
    return self.ejecutar_query(query)
```

**T√âCNICAS SQL:**
- **EXTRACT(DOW)**: Day Of Week (0=Domingo, 6=S√°bado)
- **CASE WHEN**: Condicional para contar estados espec√≠ficos
- **Tasa de asistencia**: (completadas / total) * 100

---

## üß† M√≥dulo: Predictor (predictor.py)

### Prop√≥sito
Usa redes neuronales para predecir patrones y hacer an√°lisis predictivos.

### Clase Principal: `PetStorePredictor`

```python
class PetStorePredictor:
    def __init__(self):
        self.model_tipo_mascota = None
        self.model_asistencia = None
        self.label_encoder_tipo = LabelEncoder()
        self.scaler = StandardScaler()
        self.trained = False
```

---

### Red Neuronal 1: Predicci√≥n de Tipo de Mascota

#### M√©todo: `preparar_datos_tipo_mascota(df)`

**QU√â HACE:**
- Prepara datos para entrenar la red neuronal
- Convierte texto a n√∫meros
- Normaliza valores

**C√ìDIGO:**
```python
def preparar_datos_tipo_mascota(self, df: pd.DataFrame) -> Tuple:
    # 1. Limpiar datos nulos
    df_clean = df.dropna(subset=['tipo_mascota', 'dia_semana', 'hora', 'mes'])
    
    # 2. Seleccionar features (X)
    X = df_clean[['dia_semana', 'hora', 'mes', 'service_id']].values
    
    # 3. Seleccionar target (y)
    y = self.label_encoder_tipo.fit_transform(df_clean['tipo_mascota'])
    
    # 4. Dividir en train/test (80%/20%)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # 5. Escalar features (normalizaci√≥n)
    X_train = self.scaler.fit_transform(X_train)
    X_test = self.scaler.transform(X_test)
    
    # 6. Convertir y a categorical (one-hot encoding)
    y_train_cat = keras.utils.to_categorical(y_train)
    y_test_cat = keras.utils.to_categorical(y_test)
    
    return X_train, X_test, y_train_cat, y_test_cat
```

**POR QU√â CADA PASO:**

1. **dropna()**: Elimina filas con valores nulos (la red no puede procesarlos)

2. **Features (X)**: Variables que la red usa para predecir
   - `dia_semana`: 0-6 (patrones temporales)
   - `hora`: 0-23 (patrones horarios)
   - `mes`: 1-12 (estacionalidad)
   - `service_id`: tipo de servicio

3. **LabelEncoder**: Convierte texto a n√∫meros
   ```
   "Perro" ‚Üí 0
   "Gato"  ‚Üí 1
   "Ave"   ‚Üí 2
   ```

4. **train_test_split**: Divide datos
   - 80% para entrenar
   - 20% para probar
   - `stratify=y`: Mantiene proporciones de clases

5. **StandardScaler**: Normaliza valores
   ```
   hora = 10 ‚Üí (10 - mean) / std = 0.5
   ```
   **POR QU√â:** Las redes neuronales funcionan mejor con valores normalizados

6. **to_categorical**: One-hot encoding
   ```
   Clase 0 ‚Üí [1, 0, 0]
   Clase 1 ‚Üí [0, 1, 0]
   Clase 2 ‚Üí [0, 0, 1]
   ```

---

#### M√©todo: `construir_modelo_tipo_mascota()`

**QU√â HACE:**
- Define la arquitectura de la red neuronal

**C√ìDIGO:**
```python
def construir_modelo_tipo_mascota(self, num_features: int, num_classes: int):
    model = Sequential([
        # Capa 1: Entrada y primera capa oculta
        Dense(128, activation='relu', input_shape=(num_features,)),
        Dropout(0.3),
        
        # Capa 2: Segunda capa oculta
        Dense(64, activation='relu'),
        Dropout(0.3),
        
        # Capa 3: Tercera capa oculta
        Dense(32, activation='relu'),
        Dropout(0.2),
        
        # Capa 4: Salida
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model
```

**EXPLICACI√ìN DE CADA CAPA:**

**Capa 1: Dense(128, activation='relu')**
- 128 neuronas
- ReLU: max(0, x) - No linealidad
- Aprende patrones simples

**Dropout(0.3)**
- Apaga 30% de neuronas aleatoriamente
- Previene overfitting (memorizaci√≥n)

**Capa 2: Dense(64, activation='relu')**
- 64 neuronas
- Aprende patrones m√°s complejos

**Capa 3: Dense(32, activation='relu')**
- 32 neuronas
- Refina patrones

**Capa 4: Dense(num_classes, activation='softmax')**
- Una neurona por cada clase (Perro, Gato, Ave...)
- Softmax: Convierte a probabilidades que suman 1
```
Output: [0.78, 0.15, 0.05, 0.02]
        ‚¨ÜÔ∏è    ‚¨ÜÔ∏è    ‚¨ÜÔ∏è    ‚¨ÜÔ∏è
      Perro Gato  Ave  Otros
```

**Compilaci√≥n:**
- **optimizer='adam'**: Algoritmo de optimizaci√≥n (aprende autom√°ticamente)
- **loss='categorical_crossentropy'**: Funci√≥n de p√©rdida para clasificaci√≥n
- **metrics=['accuracy']**: Mide precisi√≥n

---

#### M√©todo: `entrenar_modelo_tipo_mascota(df)`

**QU√â HACE:**
- Entrena la red neuronal con los datos

**C√ìDIGO:**
```python
def entrenar_modelo_tipo_mascota(self, df: pd.DataFrame) -> Dict:
    # 1. Preparar datos
    X_train, X_test, y_train, y_test = self.preparar_datos_tipo_mascota(df)
    
    # 2. Construir modelo
    self.model_tipo_mascota = self.construir_modelo_tipo_mascota(
        X_train.shape[1],  # N√∫mero de features
        y_train.shape[1]   # N√∫mero de clases
    )
    
    # 3. Callbacks (mejoras durante entrenamiento)
    early_stop = keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    # 4. Entrenar
    history = self.model_tipo_mascota.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stop],
        verbose=1
    )
    
    # 5. Evaluar
    y_pred = self.model_tipo_mascota.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)
    
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    
    return {"accuracy": accuracy, "history": history.history}
```

**EXPLICACI√ìN:**

**EarlyStopping:**
- Monitorea la p√©rdida en validaci√≥n
- Si no mejora en 10 √©pocas consecutivas (patience=10), detiene
- Restaura el mejor modelo

**fit():**
- **epochs=100**: M√°ximo 100 pasadas por todos los datos
- **batch_size=32**: Procesa 32 ejemplos a la vez
- **validation_split=0.2**: Usa 20% para validar durante entrenamiento

**Proceso de Entrenamiento:**
```
√âpoca 1: loss: 1.5, val_loss: 1.3, accuracy: 45%
√âpoca 2: loss: 1.2, val_loss: 1.1, accuracy: 52%
...
√âpoca 50: loss: 0.3, val_loss: 0.4, accuracy: 85%
```

---

#### M√©todo: `predecir_tipo_mascota()`

**QU√â HACE:**
- Usa el modelo entrenado para hacer predicciones

**C√ìDIGO:**
```python
def predecir_tipo_mascota(self, dia_semana: int, hora: int, 
                         mes: int, service_id: int) -> Dict:
    # 1. Crear array de features
    X = np.array([[dia_semana, hora, mes, service_id]])
    
    # 2. Escalar (usar mismo scaler del entrenamiento)
    X_scaled = self.scaler.transform(X)
    
    # 3. Predecir con la red neuronal
    pred = self.model_tipo_mascota.predict(X_scaled, verbose=0)[0]
    
    # 4. Obtener top 3
    top_indices = np.argsort(pred)[-3:][::-1]
    
    predicciones = []
    for idx in top_indices:
        predicciones.append({
            "tipo_mascota": self.label_encoder_tipo.classes_[idx],
            "probabilidad": float(pred[idx])
        })
    
    return {
        "predicciones": predicciones,
        "tipo_mas_probable": predicciones[0]["tipo_mascota"],
        "confianza": predicciones[0]["probabilidad"]
    }
```

**EJEMPLO:**
```python
# Input
predecir_tipo_mascota(
    dia_semana=5,  # Viernes
    hora=10,
    mes=11,
    service_id=1
)

# Output
{
    "predicciones": [
        {"tipo_mascota": "Perro", "probabilidad": 0.785},
        {"tipo_mascota": "Gato", "probabilidad": 0.152},
        {"tipo_mascota": "Ave", "probabilidad": 0.043}
    ],
    "tipo_mas_probable": "Perro",
    "confianza": 0.785
}
```

---

## üí¨ M√≥dulo: Chatbot (chatbot.py)

### Prop√≥sito
Chatbot inteligente que responde preguntas sobre veterinaria usando red neuronal LSTM.

### Clase Principal: `PetStoreBot`

```python
class PetStoreBot:
    def __init__(self):
        self.db = PetStoreDatabase()
        self.predictor = PetStorePredictor()
        
        # Red neuronal para el chatbot
        self.chatbot_model = None
        self.tokenizer = None
        self.label_encoder = None
        self.intents = {}
        
        # Cargar modelo entrenado
        self.cargar_modelo_chatbot()
```

---

### Red Neuronal del Chatbot: LSTM

**DIFERENCIA CON LA RED DE PREDICCI√ìN:**
- Red de Predicci√≥n: Dense layers (datos num√©ricos)
- Red del Chatbot: LSTM (secuencias de texto)

**ARQUITECTURA:**
```
Input: "mi perro tiene fiebre"
   ‚Üì
Tokenizaci√≥n: [45, 12, 3, 67]
   ‚Üì
Embedding: [[0.2, 0.5, ...], [0.8, 0.1, ...], ...]
   ‚Üì
Bidirectional LSTM: Procesa secuencia ‚Üí vectores
   ‚Üì
Dense Layers: Aprende patrones
   ‚Üì
Softmax: Probabilidades por intenci√≥n
   ‚Üì
Output: {"saludo": 0.05, "enfermedad": 0.85, ...}
```

---

#### M√©todo: `predecir_intencion_neuronal(texto)`

**QU√â HACE:**
- Clasifica la intenci√≥n del usuario usando la red neuronal

**C√ìDIGO:**
```python
def predecir_intencion_neuronal(self, texto: str) -> Tuple[str, float]:
    # 1. Normalizar texto
    texto_norm = self.normalizar_texto(texto)  # "mi perro tiene fiebre"
    
    # 2. Tokenizar: convertir palabras a n√∫meros
    sequence = self.tokenizer.texts_to_sequences([texto_norm])
    # [[45, 12, 3, 67]]
    
    # 3. Padding: rellenar/truncar a longitud fija
    padded = pad_sequences(sequence, maxlen=50, padding='post')
    # [[45, 12, 3, 67, 0, 0, 0, ...]] (50 elementos)
    
    # 4. Predecir con red neuronal
    prediction = self.chatbot_model.predict(padded, verbose=0)[0]
    # [0.02, 0.03, 0.85, 0.05, ...]  (probabilidades)
    
    # 5. Obtener intenci√≥n con mayor probabilidad
    max_confidence = float(np.max(prediction))  # 0.85
    predicted_class = np.argmax(prediction)     # 2
    
    # 6. Verificar threshold de confianza
    if max_confidence < 0.6:
        return "desconocido", max_confidence
    
    # 7. Convertir √≠ndice a etiqueta
    intent = self.label_encoder.inverse_transform([predicted_class])[0]
    # 2 ‚Üí "enfermedad_perros"
    
    return intent, max_confidence  # ("enfermedad_perros", 0.85)
```

**PROCESO COMPLETO:**
```
"mi perro tiene fiebre"
   ‚Üì normalizar
"mi perro tiene fiebre"
   ‚Üì tokenizar
[45, 12, 3, 67]
   ‚Üì padding
[45, 12, 3, 67, 0, 0, ..., 0]  (50 elementos)
   ‚Üì red neuronal
[0.02, 0.03, 0.85, 0.05, ...]  (probabilidades)
   ‚Üì argmax + decoder
("enfermedad_perros", 0.85)
```

---

#### M√©todo: `procesar_mensaje(mensaje)`

**QU√â HACE:**
- Procesa mensaje del usuario y genera respuesta

**C√ìDIGO:**
```python
def procesar_mensaje(self, mensaje: str) -> Dict:
    # 1. Detectar intenci√≥n con red neuronal
    intencion, confianza = self.predecir_intencion_neuronal(mensaje)
    
    # 2. Si es intenci√≥n veterinaria, responder de inmediato
    if intencion in self.intents:
        respuestas = self.intents[intencion]
        respuesta = random.choice(respuestas)
    
    # 3. Si es consulta de datos, consultar BD
    elif intencion == 'tipo_mascota_comun':
        respuesta = self.responder_tipo_mas_comun()
    
    elif intencion == 'dia_mas_atencion':
        respuesta = self.responder_dia_mas_atencion()
    
    # 4. Si no entiende, respuesta por defecto
    else:
        respuesta = "No entend√≠ tu pregunta..."
    
    return {
        "respuesta": respuesta,
        "intencion": intencion,
        "confianza": confianza,
        "timestamp": datetime.now().isoformat()
    }
```

**FLUJO:**
```
Usuario: "mi perro tiene fiebre"
   ‚Üì
Red Neuronal: intencion="enfermedad_perros", confianza=0.85
   ‚Üì
Buscar respuesta en self.intents["enfermedad_perros"]
   ‚Üì
Retornar: "Las enfermedades m√°s comunes en perros incluyen..."
```

---

## üèãÔ∏è Entrenamiento del Chatbot

### Script: `entrenar_chatbot_veterinario.py`

**PROCESO COMPLETO:**

#### PASO 1: Cargar Datos

```python
# Leer archivo JSON con intenciones
with open('datos_veterinarios.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Extraer patrones y etiquetas
for intent in data['intents']:
    tag = intent['tag']  # "enfermedad_perros"
    for pattern in intent['patterns']:
        patterns.append(pattern)  # "mi perro est√° enfermo"
        labels.append(tag)
```

**Datos:**
```json
{
  "tag": "enfermedad_perros",
  "patterns": [
    "mi perro est√° enfermo",
    "enfermedades comunes en perros",
    "mi perro tiene fiebre"
  ],
  "responses": [
    "Las enfermedades m√°s comunes en perros incluyen..."
  ]
}
```

#### PASO 2: Tokenizaci√≥n

```python
# Crear tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(patterns)

# Convertir textos a secuencias
sequences = tokenizer.texts_to_sequences(patterns)

# Padding
X = pad_sequences(sequences, maxlen=50, padding='post')
```

**Ejemplo:**
```
"mi perro est√° enfermo"
   ‚Üì fit_on_texts (construye vocabulario)
word_index = {"mi": 1, "perro": 2, "est√°": 3, "enfermo": 4, ...}
   ‚Üì texts_to_sequences
[1, 2, 3, 4]
   ‚Üì pad_sequences
[1, 2, 3, 4, 0, 0, 0, ..., 0]  (50 elementos)
```

#### PASO 3: Codificar Etiquetas

```python
# Convertir etiquetas a n√∫meros
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(labels)

# One-hot encoding
y = keras.utils.to_categorical(y_encoded)
```

**Ejemplo:**
```
labels = ["saludo", "enfermedad", "saludo", "vacuna"]
   ‚Üì fit_transform
y_encoded = [0, 1, 0, 2]
   ‚Üì to_categorical
y = [[1, 0, 0],
     [0, 1, 0],
     [1, 0, 0],
     [0, 0, 1]]
```

#### PASO 4: Construir Red Neuronal LSTM

```python
model = Sequential([
    # Embedding: palabras ‚Üí vectores densos
    Embedding(
        input_dim=5000,      # Tama√±o vocabulario
        output_dim=128,      # Dimensi√≥n del vector
        input_length=50      # Longitud secuencia
    ),
    
    # LSTM Bidireccional: procesa en ambas direcciones
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    
    # Segunda LSTM
    Bidirectional(LSTM(32)),
    Dropout(0.5),
    
    # Capas densas
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    
    # Salida: una neurona por intenci√≥n
    Dense(num_classes, activation='softmax')
])
```

**POR QU√â LSTM Y NO DENSE:**
- LSTM mantiene memoria de palabras anteriores
- Entiende contexto: "no me siento bien" vs "me siento bien"
- Bidirectional lee en ambas direcciones

#### PASO 5: Entrenar

```python
history = model.fit(
    X_train, y_train,
    epochs=150,
    batch_size=8,
    validation_split=0.2,
    callbacks=[early_stop, reduce_lr]
)
```

#### PASO 6: Guardar Modelo

```python
model.save('models/chatbot_veterinario.h5')
pickle.dump(tokenizer, open('models/tokenizer_veterinario.pkl', 'wb'))
pickle.dump(label_encoder, open('models/label_encoder_veterinario.pkl', 'wb'))
```

---

## üîå API REST (api.py)

### Prop√≥sito
Expone todos los m√≥dulos como endpoints HTTP para consumo desde frontend.

### Endpoint Principal: `POST /api/chat`

```python
@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    # 1. Recibir mensaje del frontend
    mensaje = request.mensaje  # "mi perro tiene fiebre"
    
    # 2. Procesar con el chatbot
    resultado = bot.procesar_mensaje(mensaje)
    # {"respuesta": "...", "intencion": "...", "confianza": 0.85}
    
    # 3. Retornar como JSON
    return ChatResponse(**resultado)
```

**FLUJO COMPLETO:**
```
Frontend ‚Üí POST /api/chat
            ‚Üì
         api.py recibe
            ‚Üì
         bot.procesar_mensaje()
            ‚Üì
         Red Neuronal clasifica
            ‚Üì
         Genera respuesta
            ‚Üì
         api.py retorna JSON
            ‚Üì
         Frontend muestra
```

---

## üéì C√≥mo Entrenar los Modelos

### 1. Entrenar Chatbot Veterinario

```bash
python entrenar_chatbot_veterinario.py
```

**QU√â HACE:**
- Lee `datos_veterinarios.json`
- Entrena red neuronal LSTM
- Guarda modelo en `models/chatbot_veterinario.h5`
- Tarda: 2-5 minutos

**AGREGAR M√ÅS DATOS:**
Edita `datos_veterinarios.json`:
```json
{
  "tag": "nueva_intencion",
  "patterns": [
    "pregunta 1",
    "pregunta 2"
  ],
  "responses": [
    "respuesta"
  ]
}
```

### 2. Entrenar Predictor de Datos

**Opci√≥n A: Desde API**
```bash
curl -X POST http://localhost:8000/api/entrenar
```

**Opci√≥n B: Desde Python**
```bash
python
>>> from database import PetStoreDatabase
>>> from predictor import PetStorePredictor
>>> db = PetStoreDatabase()
>>> pred = PetStorePredictor()
>>> df = db.obtener_dataset_completo()
>>> pred.entrenar_modelo_tipo_mascota(df)
>>> pred.guardar_modelos()
```

---

## üéØ Resumen de Flujo de Datos

### Consulta Simple (Estad√≠sticas)
```
Frontend ‚Üí API ‚Üí database.py ‚Üí PostgreSQL ‚Üí DataFrame ‚Üí JSON ‚Üí Frontend
```

### Predicci√≥n (Red Neuronal)
```
Frontend ‚Üí API ‚Üí predictor.py ‚Üí Red Neuronal ‚Üí Predicci√≥n ‚Üí JSON ‚Üí Frontend
```

### Chatbot (Red Neuronal LSTM)
```
Frontend ‚Üí API ‚Üí chatbot.py
                    ‚Üì
                Red Neuronal LSTM
                    ‚Üì
            Clasificaci√≥n Intenci√≥n
                    ‚Üì
            Respuesta ‚Üí JSON ‚Üí Frontend
```

---

## üìù Conceptos Clave

### ¬øQu√© es una Red Neuronal?
Modelo matem√°tico inspirado en el cerebro que aprende patrones de los datos.

### ¬øC√≥mo aprende?
1. Hace predicci√≥n
2. Mide error
3. Ajusta pesos
4. Repite hasta minimizar error

### ¬øPor qu√© LSTM para texto?
- Mantiene memoria de secuencias
- Entiende contexto
- Mejor que Dense para texto

### ¬øPor qu√© Dense para n√∫meros?
- M√°s simple
- Suficiente para features num√©ricos
- M√°s r√°pido

---

## ‚úÖ Checklist de Entrenamiento

- [ ] Datos suficientes en BD (>100 registros)
- [ ] Ejecutar `python entrenar_chatbot_veterinario.py`
- [ ] Verificar archivos en `models/`
- [ ] Iniciar API: `python api.py`
- [ ] Probar endpoint de chat
- [ ] ¬°Listo!

---

**Documentaci√≥n completa del sistema PetStore - Noviembre 2024**

