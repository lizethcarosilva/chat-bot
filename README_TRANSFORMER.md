# ğŸ¤– Chatbot con Transformer - Pet Store

## ğŸ“‹ DescripciÃ³n

Sistema de chatbot inteligente que utiliza **arquitectura Transformer** (similar a GPT) para generar respuestas contextuales y naturales. El sistema combina:

- **Red Neuronal Transformer** con Multi-Head Attention
- **GeneraciÃ³n autoregresiva** de texto
- **Enriquecimiento con datos** en tiempo real de la base de datos
- **Respuestas contextuales** basadas en el historial de conversaciÃ³n

## ğŸ—ï¸ Arquitectura del Transformer

### Componentes Principales

```
Input â†’ Embedding â†’ Positional Encoding
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Transformer Block 1       â”‚
        â”‚   - Multi-Head Attention    â”‚
        â”‚   - Feed-Forward Network    â”‚
        â”‚   - Layer Normalization     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Transformer Block N       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
              Linear â†’ Softmax â†’ Output
```

### ParÃ¡metros del Modelo

- **d_model**: 256 (dimensiÃ³n de embeddings)
- **num_heads**: 8 (cabezas de atenciÃ³n)
- **num_layers**: 4 (bloques transformer)
- **d_ff**: 1024 (dimensiÃ³n feed-forward)
- **vocab_size**: ~5000 palabras
- **max_length**: 128 tokens

## ğŸš€ InstalaciÃ³n

### 1. Instalar Dependencias

```bash
pip install -r requirements.txt
```

Esto instalarÃ¡:
- **PyTorch** (framework de deep learning)
- **TensorFlow** (para LSTM legacy)
- **FastAPI** (API REST)
- Otras dependencias necesarias

### 2. Verificar InstalaciÃ³n

```bash
python -c "import torch; print(f'PyTorch {torch.__version__} instalado correctamente')"
```

## ğŸ“š Entrenamiento del Modelo

### OpciÃ³n 1: Entrenamiento Completo

```bash
python entrenar_transformer.py
```

Este script:
1. âœ… Genera datos de entrenamiento (pares pregunta-respuesta)
2. âœ… Construye el vocabulario
3. âœ… Entrena el modelo Transformer
4. âœ… Guarda el modelo en `models/transformer_chatbot.pth`

**Tiempo estimado**: 10-30 minutos (dependiendo de epochs y hardware)

### OpciÃ³n 2: Usar Modelo Pre-entrenado

Si no deseas entrenar desde cero, el sistema funciona en **modo hÃ­brido**:
- Detecta intenciones con patrones
- Enriquece respuestas con datos reales
- Genera respuestas contextuales

## ğŸ¯ Uso del Chatbot

### Desde la API REST

```bash
# Iniciar servidor
python api.py
```

Luego desde tu frontend o herramienta (Postman, curl, etc.):

```javascript
// Ejemplo con JavaScript/Fetch
const response = await fetch('http://localhost:8000/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        mensaje: "Â¿CuÃ¡l es el tipo de mascota mÃ¡s comÃºn?",
        usuario_id: "user123"
    })
});

const data = await response.json();
console.log(data.respuesta);
console.log(`Modelo usado: ${data.modelo}`);
console.log(`Confianza: ${data.confianza * 100}%`);
```

```bash
# Ejemplo con curl
curl -X POST "http://localhost:8000/api/chat" \
     -H "Content-Type: application/json" \
     -d '{"mensaje": "MuÃ©strame las estadÃ­sticas"}'
```

### Alternar entre Modelos

Puedes elegir entre el **Transformer** (nuevo) o **LSTM** (clÃ¡sico):

```javascript
// Usar Transformer (por defecto)
fetch('http://localhost:8000/api/chat?use_transformer=true', {...})

// Usar LSTM clÃ¡sico
fetch('http://localhost:8000/api/chat?use_transformer=false', {...})
```

### Desde Python Directamente

```python
from transformer_chatbot import PetStoreBotTransformer

bot = PetStoreBotTransformer()

resultado = bot.procesar_mensaje("Hola, Â¿cÃ³mo estÃ¡s?")
print(resultado['respuesta'])
print(f"Confianza: {resultado['confianza']:.0%}")
print(f"Modelo: {resultado['modelo']}")
```

## ğŸ’¬ Capacidades del Chatbot

### Consultas Soportadas

#### ğŸ“Š EstadÃ­sticas y MÃ©tricas
- "MuÃ©strame las estadÃ­sticas"
- "Â¿CuÃ¡ntas mascotas hay registradas?"
- "Dame un reporte del negocio"
- "MÃ©tricas actuales"

#### ğŸ“… Citas y Agenda
- "Â¿CuÃ¡ntas citas hay hoy?"
- "MuÃ©strame la agenda"
- "PrÃ³ximas citas programadas"

#### ğŸ’° Ventas e Ingresos
- "Â¿CuÃ¡nto vendimos hoy?"
- "Ventas del mes"
- "Reporte de ingresos"
- "Comparativa con mes anterior"

#### ğŸ¾ InformaciÃ³n de Mascotas
- "Â¿CuÃ¡l es el tipo de mascota mÃ¡s comÃºn?"
- "Tipos de mascotas registradas"
- "BÃºsqueda de mascota por nombre"

#### ğŸ“¦ Inventario y Productos
- "Â¿CuÃ¡ntos productos tenemos?"
- "Productos prÃ³ximos a vencer"
- "Alertas de bajo stock"
- "Inventario actual"

#### ğŸ”® Predicciones con IA
- "Predice el tipo de mascota"
- "AnÃ¡lisis predictivo"

#### ğŸ”¬ Clustering y SegmentaciÃ³n
- "AnÃ¡lisis de clustering"
- "SegmentaciÃ³n de clientes"
- "Agrupa mascotas por caracterÃ­sticas"

## ğŸ¨ Respuesta del API

### Estructura de Respuesta

```json
{
  "respuesta": "ğŸ“Š **EstadÃ­sticas del Sistema:**\n\nğŸ¾ Mascotas registradas: 150...",
  "intencion": "transformer_generation",
  "confianza": 0.85,
  "timestamp": "2024-11-06T10:30:00",
  "modelo": "Transformer"
}
```

### Campos

- **respuesta**: Texto generado por el bot (puede incluir Markdown)
- **intencion**: Tipo de intenciÃ³n detectada
- **confianza**: Nivel de confianza (0.0 - 1.0)
- **timestamp**: Momento de la respuesta
- **modelo**: Modelo usado ("Transformer" o "HÃ­brido")

## âš™ï¸ ConfiguraciÃ³n

### Archivo: `config_transformer.py`

Puedes modificar:

```python
TRANSFORMER_CONFIG = {
    'd_model': 256,          # DimensiÃ³n del modelo
    'num_heads': 8,          # Cabezas de atenciÃ³n
    'num_layers': 4,         # Bloques transformer
    'temperature': 0.8,      # Creatividad (0.1-1.5)
    'top_k': 50,             # Top-K sampling
    'max_generate_length': 100,  # Longitud mÃ¡xima
}
```

### Ajustar Temperatura

- **temperature = 0.1**: Respuestas mÃ¡s deterministas y consistentes
- **temperature = 0.8**: Balance (recomendado)
- **temperature = 1.5**: Respuestas mÃ¡s creativas y variadas

## ğŸ§ª Testing

### Probar el Modelo

```bash
python transformer_chatbot.py
```

Esto ejecutarÃ¡ ejemplos de prueba automÃ¡ticamente.

### Endpoints de Prueba

```bash
# Health check
curl http://localhost:8000/api/health

# Comandos disponibles
curl http://localhost:8000/api/chat/comandos

# Estado del modelo
curl http://localhost:8000/api/predicciones/estado
```

## ğŸ“ˆ Performance

### MÃ©tricas Esperadas

| MÃ©trica | Valor |
|---------|-------|
| Tiempo de respuesta | < 1s |
| Confianza promedio | 75-90% |
| Vocabulario | ~5000 palabras |
| ParÃ¡metros del modelo | ~2M |

### Optimizaciones

- âœ… **CachÃ© de respuestas** frecuentes
- âœ… **Batch processing** para mÃºltiples consultas
- âœ… **GPU support** (si disponible)
- âœ… **Modo hÃ­brido** como fallback

## ğŸ”§ Troubleshooting

### Problema: Modelo no encontrado

**SoluciÃ³n**: Entrena el modelo primero
```bash
python entrenar_transformer.py
```

### Problema: Out of Memory (OOM)

**SoluciÃ³n**: Reduce el batch_size en `config_transformer.py`
```python
'batch_size': 16  # En lugar de 32
```

### Problema: Respuestas repetitivas

**SoluciÃ³n**: Aumenta la temperature
```python
'temperature': 1.0  # En lugar de 0.8
```

### Problema: Respuestas inconsistentes

**SoluciÃ³n**: Reduce la temperature
```python
'temperature': 0.5  # En lugar de 0.8
```

## ğŸ“Š ComparaciÃ³n: Transformer vs LSTM

| CaracterÃ­stica | Transformer | LSTM |
|----------------|-------------|------|
| Arquitectura | Multi-Head Attention | Recurrente |
| ParalelizaciÃ³n | âœ… Excelente | âŒ Limitada |
| Contexto | âœ… Largo alcance | âš ï¸ Corto |
| Respuestas | âœ… MÃ¡s naturales | âš ï¸ Predefinidas |
| Entrenamiento | âš ï¸ MÃ¡s lento | âœ… MÃ¡s rÃ¡pido |
| Memoria | âš ï¸ Mayor | âœ… Menor |

## ğŸ“ Conceptos TÃ©cnicos

### Multi-Head Attention

Permite al modelo enfocarse en diferentes partes de la entrada simultÃ¡neamente:

```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

### Positional Encoding

Agrega informaciÃ³n de posiciÃ³n a los embeddings:

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### GeneraciÃ³n Autoregresiva

El modelo genera una palabra a la vez, usando las anteriores como contexto:

```
Input: "Hola" â†’ Output: "Â¡Hola!"
Input: "Â¡Hola! Estoy" â†’ Output: "aquÃ­"
Input: "Â¡Hola! Estoy aquÃ­" â†’ Output: "para"
...
```

## ğŸ” Seguridad

- âœ… SanitizaciÃ³n de entradas
- âœ… ValidaciÃ³n de longitud
- âœ… Rate limiting (FastAPI)
- âœ… CORS configurado

## ğŸ“ Logs

Los logs se guardan en:
- `logs/transformer_chatbot.log`

Ver logs en tiempo real:
```bash
tail -f logs/transformer_chatbot.log
```

## ğŸš€ ProducciÃ³n

### Consideraciones

1. **Usar GPU**: Mejora el rendimiento 10-50x
2. **CachÃ© Redis**: Para respuestas frecuentes
3. **Load Balancer**: Para mÃºltiples instancias
4. **Monitoreo**: Prometheus + Grafana

### Deploy con Docker

```dockerfile
FROM python:3.10

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ“š Referencias

- [Attention Is All You Need (Paper original)](https://arxiv.org/abs/1706.03762)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

## ğŸ¤ Contribuciones

Para mejorar el modelo:

1. Agrega mÃ¡s datos de entrenamiento en `entrenar_transformer.py`
2. Ajusta hiperparÃ¡metros en `config_transformer.py`
3. Experimenta con diferentes arquitecturas

## ğŸ“„ Licencia

Este proyecto es para fines educativos.

## ğŸ‘¥ Soporte

Si tienes dudas:
1. Revisa la documentaciÃ³n tÃ©cnica en `/Docs`
2. Consulta los ejemplos en el cÃ³digo
3. Revisa los logs para debugging

---

**Hecho con â¤ï¸ y ğŸ¤– Transformers**

